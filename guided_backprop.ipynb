{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided Backpropagation\n",
    "\n",
    "Guided backpropagation is a combination of the gradient and deconvolution attribution methods. The deconvolution method discussed in [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034), was shown to be equivalent to a backward pass through the network, except for its interaction with the relu layer. The negative gradient values would be zeroed by relu, but the gradient values coming from negative input image values were not zeroed. Guided backpropagation method adapts this idea by **zeroing both negative gradients and gradients coming from negative input values**. We will implement this method by introducing a new relu layer called guided relu. Unfortunately guided relu performs poorly when training, so the model will first be trained with relu layers, then the relu layers will be swapped out for the guided relu layers when evaluating the attribution maps. There is no obvious way to swap layers in flax, so instead we will define a new model with the same architecture but guided relu inplace of relu layers.  \n",
    "\n",
    "For more technical information on the guided backpropagation attribution method see: [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "from jax import custom_vjp\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "from jax import random\n",
    "from jax import grad\n",
    "import optax\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import flaxmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "Check if the training data exists. If not, automatically download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.abspath(\"./digit-recognizer/datasets\")\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "train_data_path = f\"{data_dir}/train.zip\"\n",
    "train_data_url = \"https://huggingface.co/datasets/ChristianOrr/mnist/resolve/main/train.zip\"\n",
    "\n",
    "def download_file(url, dest_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "    with open(dest_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            file.write(chunk)\n",
    "\n",
    "# Download the training data if it doesn't already exist\n",
    "if not os.path.exists(train_data_path):\n",
    "    print(\"Downloading training data...\")\n",
    "    download_file(train_data_url, train_data_path)\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(train_data_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "\n",
    "data_df = pd.read_csv('./digit-recognizer/datasets/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 1000\n",
    "# Only shuffle training data\n",
    "np.random.shuffle(data_df[val_size:].values)\n",
    "m, n = data_df.shape\n",
    "Y = jnp.array(data_df[\"label\"])\n",
    "data_df = data_df.drop(\"label\", axis=1)\n",
    "X = jnp.array(data_df)\n",
    "\n",
    "\n",
    "X_train = X[val_size:]\n",
    "X_train = X_train / 255.\n",
    "Y_train = Y[val_size:]\n",
    "\n",
    "X_val = X[:val_size]\n",
    "X_val = X_val / 255.\n",
    "Y_val = Y[:val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Incompatible shapes for broadcasting: shapes=[(5, 784), (1, 1, 1, 3)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/util.py:311\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m trace_context_in_key:\n\u001b[0;32m--> 311\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/util.py:304\u001b[0m, in \u001b[0;36mcache.<locals>.wrap.<locals>.cached\u001b[0;34m(_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(max_size)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached\u001b[39m(_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 304\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/lax/lax.py:156\u001b[0m, in \u001b[0;36m_broadcast_shapes_cached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;129m@cache\u001b[39m()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_broadcast_shapes_cached\u001b[39m(\u001b[38;5;241m*\u001b[39mshapes: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]:\n\u001b[0;32m--> 156\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_broadcast_shapes_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/lax/lax.py:172\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(5, 784), (1, 1, 1, 3)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m rng_key \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# initial_params = model.init(rng_key, dummy_x)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtabulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdummy_x\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# print(model.tabulate(rng_key, dummy_x))\u001b[39;00m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/flax/linen/module.py:2861\u001b[0m, in \u001b[0;36mModule.tabulate\u001b[0;34m(self, rngs, depth, show_repeated, mutable, console_kwargs, table_kwargs, column_kwargs, compute_flops, compute_vjp_flops, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2847\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinen\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m   2849\u001b[0m tabulate_fn \u001b[38;5;241m=\u001b[39m summary\u001b[38;5;241m.\u001b[39mtabulate(\n\u001b[1;32m   2850\u001b[0m   \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2851\u001b[0m   rngs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2859\u001b[0m   compute_vjp_flops\u001b[38;5;241m=\u001b[39mcompute_vjp_flops,\n\u001b[1;32m   2860\u001b[0m )\n\u001b[0;32m-> 2861\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtabulate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/flax/linen/summary.py:315\u001b[0m, in \u001b[0;36mtabulate.<locals>._tabulate_fn\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tabulate_fn\u001b[39m(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs):\n\u001b[1;32m    307\u001b[0m   table_fn \u001b[38;5;241m=\u001b[39m _get_module_table(\n\u001b[1;32m    308\u001b[0m     module,\n\u001b[1;32m    309\u001b[0m     depth\u001b[38;5;241m=\u001b[39mdepth,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m     compute_vjp_flops\u001b[38;5;241m=\u001b[39mcompute_vjp_flops,\n\u001b[1;32m    313\u001b[0m   )\n\u001b[0;32m--> 315\u001b[0m   table \u001b[38;5;241m=\u001b[39m \u001b[43mtable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrngs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m   non_param_cols \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    322\u001b[0m   ]\n\u001b[1;32m    324\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m compute_flops:\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/flax/linen/summary.py:450\u001b[0m, in \u001b[0;36m_get_module_table.<locals>._get_table_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_variables\u001b[39m():\n\u001b[1;32m    448\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39minit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 450\u001b[0m variables \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m calls \u001b[38;5;241m=\u001b[39m module_lib\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mcall_info_stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcalls\n\u001b[1;32m    452\u001b[0m calls\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m c: c\u001b[38;5;241m.\u001b[39mindex)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/flax/linen/summary.py:448\u001b[0m, in \u001b[0;36m_get_module_table.<locals>._get_table_fn.<locals>._get_variables\u001b[0;34m()\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_variables\u001b[39m():\n\u001b[0;32m--> 448\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/flaxmodels/resnet/resnet.py:300\u001b[0m, in \u001b[0;36mResNet.__call__\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m    298\u001b[0m     mean \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    299\u001b[0m     std \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 300\u001b[0m     x \u001b[38;5;241m=\u001b[39m (\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m) \u001b[38;5;241m/\u001b[39m std\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:265\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    263\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 265\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/numpy/ufuncs.py:86\u001b[0m, in \u001b[0;36m_one_to_one_binop.<locals>.<lambda>\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m     84\u001b[0m   fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2, \u001b[38;5;241m/\u001b[39m: lax_fn(\u001b[38;5;241m*\u001b[39mpromote_args_numeric(numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, x1, x2))\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m   fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x1, x2, \u001b[38;5;241m/\u001b[39m: lax_fn(\u001b[38;5;241m*\u001b[39m\u001b[43mpromote_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumpy_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     87\u001b[0m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m numpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m     88\u001b[0m fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax.numpy.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/numpy/util.py:381\u001b[0m, in \u001b[0;36mpromote_args\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    379\u001b[0m _check_no_float0s(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    380\u001b[0m check_for_prngkeys(fun_name, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpromote_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpromote_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/numpy/util.py:250\u001b[0m, in \u001b[0;36mpromote_shapes\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mnumpy_rank_promotion\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    249\u001b[0m   _rank_promotion_warning_or_error(fun_name, shapes)\n\u001b[0;32m--> 250\u001b[0m result_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mshapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [_broadcast_to(arg, (\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m*\u001b[39m (result_rank \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(shp)) \u001b[38;5;241m+\u001b[39m shp)\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m arg, shp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, shapes)]\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/conda/miniconda/envs/ml/lib/python3.11/site-packages/jax/_src/lax/lax.py:172\u001b[0m, in \u001b[0;36m_broadcast_shapes_uncached\u001b[0;34m(*shapes)\u001b[0m\n\u001b[1;32m    170\u001b[0m result_shape \u001b[38;5;241m=\u001b[39m _try_broadcast_shapes(shape_list)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for broadcasting: shapes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(shapes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_shape\n",
      "\u001b[0;31mValueError\u001b[0m: Incompatible shapes for broadcasting: shapes=[(5, 784), (1, 1, 1, 3)]"
     ]
    }
   ],
   "source": [
    "# Pretrained on ImageNet (1000 classes)\n",
    "# Output the probabilities of the 1000 classes\n",
    "model = flaxmodels.ResNet50()\n",
    "dummy_x = X_train[:5]\n",
    "rng_key = random.PRNGKey(64)\n",
    "# initial_params = model.init(rng_key, dummy_x)\n",
    "print(model.tabulate(rng_key, dummy_x))\n",
    "# print(model.tabulate(rng_key, dummy_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _forward_fn(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Unflatten for the convolution layers\n",
    "        x = x.reshape(batch_size, 28, 28, 1)\n",
    "        conv_layers = nn.Sequential([\n",
    "            nn.Conv(features=16, kernel_size=(3, 3), padding=\"VALID\", use_bias=False), jax.nn.relu,\n",
    "            nn.Conv(features=32, kernel_size=(3, 3), padding=\"VALID\", use_bias=False), jax.nn.relu,   \n",
    "        ])\n",
    "        x = conv_layers(x)\n",
    "        # Flatten for dense layer\n",
    "        x = x.reshape((batch_size, -1))\n",
    "        x = nn.Dense(10)(x)\n",
    "        return jax.nn.softmax(x)\n",
    "\n",
    "# Create eh model object\n",
    "forward_fn = _forward_fn()\n",
    "# Display the model details\n",
    "dummy_x = X_train[:5]\n",
    "rng_key = random.PRNGKey(64)\n",
    "initial_params = forward_fn.init(rng_key, dummy_x)\n",
    "print(forward_fn.tabulate(rng_key, dummy_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the loss function and update method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, X, Y):\n",
    "    logits = forward_fn.apply(params, X)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, Y)\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def update_params(params, opt_state, X, Y, optimizer):\n",
    "    grads = grad(loss_fn, argnums=0)(params, X, Y)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(logits):\n",
    "    return jnp.argmax(logits, axis=1)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return jnp.mean(predictions == Y)\n",
    "\n",
    "def gradient_descent(X, Y, batch_size, optimizer, epochs):\n",
    "    dataset_size = len(X)\n",
    "    steps_per_epoch = dataset_size // batch_size\n",
    "\n",
    "    rng = jax.random.PRNGKey(42)\n",
    "    params = forward_fn.init(rng, X[0: batch_size, :])\n",
    "    opt_state = optimizer.init(params)\n",
    "    for epoch in range(epochs):\n",
    "        dataset_step = epoch % steps_per_epoch\n",
    "        dataset_index = dataset_step * batch_size\n",
    "        input = X[dataset_index: dataset_index + batch_size, :]\n",
    "        label = Y[dataset_index: dataset_index + batch_size, ...]\n",
    "\n",
    "        params, opt_state = update_params(params, opt_state, input, label, optimizer)\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epochs: \", epoch)\n",
    "            logits = forward_fn.apply(params, input)\n",
    "            predictions = get_predictions(logits)\n",
    "            print(get_accuracy(predictions, label))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=0.001)\n",
    "\n",
    "params = gradient_descent(X_train, Y_train, 1000, optimizer, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Guided Relu\n",
    "\n",
    "The key to the guided backprop method is the guided relu layer. We will start by defining this layer. The difficult part of this layer is getting access to the gradients and manipulating them during the backward pass. Fortunately Jax provides a way to overide the differentiation operations, more details on that here: [Custom derivative rules for JAX-transformable Python functions](https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html). The most important part of the layer is in the backward function called guided_relu_bwd. We start by taking the derivative of relu, which outputs 1 of the input is > 1 and outputs 0 otherwise. The relu derivative is then applied to both the inputs, called residual, and gradients, called grad in the function. This provides us with 0 or 1 variable which act like gates, hence the variable names grad_gate and residual_gate. The gates are then multiplied by grad to give us the final output. Alternative implementations of guided relu are available in Pytorch and Tensorflow here: [Guided Backpropagation with PyTorch and TensorFlow](https://www.coderskitchen.com/guided-backpropagation-with-pytorch-and-tensorflow/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@custom_vjp\n",
    "def guided_relu(x):\n",
    "    return jax.nn.relu(x)\n",
    "\n",
    "def guided_relu_fwd(x):\n",
    "    residual = x\n",
    "    primal = guided_relu(x)\n",
    "\n",
    "    return primal, residual\n",
    "\n",
    "def guided_relu_bwd(residual, grad):\n",
    "    # Derivative of relu is 1 for values > 0 and 0 otherwise\n",
    "    # Using relu devivative for both residuals and grads\n",
    "    grad_gate = jnp.float32(grad > 0)\n",
    "    residual_gate = jnp.float32(residual > 0)\n",
    "    output = residual_gate * grad_gate * grad\n",
    "    return (output, ) \n",
    "\n",
    "guided_relu.defvjp(guided_relu_fwd, guided_relu_bwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate guided relu on sin\n",
    "\n",
    "To demonstrate how guided relu works and if its working we will visualize its impact on the sin function. We start with a 1D input vector from -5 to 5. The input is then fed into sin and the derivative of sin and shown below. This demonstates how the output looks with no alterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = jnp.linspace(-5, 5, 1000)\n",
    "\n",
    "plt.plot(jnp.sin(input_vector))\n",
    "plt.plot(jax.vmap(grad(jnp.sin))(input_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we alter the input values by passing them first to relu. The first half of the inputs were negative. This caused the first half of the sin and derivative of sin outputs to be 0, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_sin(x):\n",
    "    x = jax.nn.relu(x)\n",
    "    return jnp.sin(x)\n",
    "\n",
    "plt.plot(relu_sin(input_vector))\n",
    "plt.plot(jax.vmap(grad(relu_sin))(input_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replace relu with guided relu. The output of the sin function is unchanged from standard relu, but now the gradients of sin have been altered further. In addition to the gradients being 0 for all negative input values, they are also 0 in places where the gradients would be zero. This demonstrates guided relu is working as desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_relu_sin(x):\n",
    "    x = guided_relu(x)\n",
    "    return jnp.sin(x)\n",
    "\n",
    "plt.plot(guided_relu_sin(input_vector))\n",
    "plt.plot(jax.vmap(grad(guided_relu_sin))(input_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the guided model\n",
    "\n",
    "Now that we have the guided relu layers we can swap them with the relu layers in the classification model. The model architecture is identical to the one trained previously so we can use its trained parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _guided_forward_fn(nn.Module):\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        # Unflatten for the convolution layers\n",
    "        x = x.reshape(batch_size, 28, 28, 1)\n",
    "        conv_layers = nn.Sequential([\n",
    "            nn.Conv(features=16, kernel_size=(3, 3), padding=\"VALID\", use_bias=False), guided_relu,\n",
    "            nn.Conv(features=32, kernel_size=(3, 3), padding=\"VALID\", use_bias=False), guided_relu,   \n",
    "        ])\n",
    "        x = conv_layers(x)\n",
    "        # Flatten for dense layer\n",
    "        x = x.reshape((batch_size, -1))\n",
    "        x = nn.Dense(10)(x)\n",
    "        return jax.nn.softmax(x)\n",
    "\n",
    "# Create eh model object\n",
    "guided_forward_fn = _guided_forward_fn()\n",
    "# Display the model details\n",
    "dummy_x = X_train[:5]\n",
    "rng_key = random.PRNGKey(64)\n",
    "initial_params = guided_forward_fn.init(rng_key, dummy_x)\n",
    "print(guided_forward_fn.tabulate(rng_key, dummy_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new loss function with guided model\n",
    "\n",
    "A new loss function containing the model with guided relu layers is required when performing gradient calculations for the attribution attribution maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guided_loss_fn(params, X, Y):\n",
    "    logits = guided_forward_fn.apply(params, X)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, Y)\n",
    "    loss = loss.mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display predictions and attribution maps\n",
    "\n",
    "The gradients are calculated with the new guided loss function. The gradients are then normalized in the standard way for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X, params):\n",
    "    logits = forward_fn.apply(params, X)\n",
    "    predictions = get_predictions(logits)\n",
    "    return predictions\n",
    "\n",
    "def prep_image(image):\n",
    "    # Normalize the gradient values to be between 0-1\n",
    "    max_val= np.max(image)\n",
    "    min_val = np.min(image)\n",
    "    image = (image - min_val) / (max_val - min_val)\n",
    "    # Convert the grads to uint8 for displaying\n",
    "    image = np.uint8(image * 255)\n",
    "    return image   \n",
    "\n",
    "def display_prediction(index, params):\n",
    "    current_image = X_val[None, index]\n",
    "    prediction = make_predictions(X_val[None, index], params)\n",
    "\n",
    "    label = Y_val[index]\n",
    "    print(\"Prediction: \", prediction)\n",
    "    print(\"Label: \", label)\n",
    "\n",
    "    display_image = current_image.reshape((28, 28)) * 255\n",
    "    plt.gray()\n",
    "    plt.imshow(display_image, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    attributions = grad(guided_loss_fn, argnums=1)(params, current_image, label[None, ...])[0]\n",
    "    attributions = attributions.reshape((28, 28))\n",
    "    attributions = prep_image(attributions)\n",
    "\n",
    "    plt.gray()\n",
    "    plt.imshow(attributions, interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Attribution Map\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_prediction(0, params)\n",
    "display_prediction(3, params)\n",
    "display_prediction(5, params)\n",
    "display_prediction(8, params)\n",
    "display_prediction(10, params)\n",
    "display_prediction(50, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('flax')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a8d1f2580cdfde5c5829808ec6fccc81a351d243fb8b1925f7928e44ccf575b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
